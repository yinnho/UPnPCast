# UPnPCast库性能比较测试方案

## 测试目标

对UPnPCast库第七阶段优化前后的性能进行全面对比，评估优化效果，确保优化没有引入新的性能问题。主要关注以下几个方面：

1. 内存占用
2. 响应时间
3. 电池消耗
4. 启动时间
5. 资源利用率

## 测试环境

- **测试设备**：
  - 低端设备：Android 7.0，2GB RAM
  - 中端设备：Android 10.0，4GB RAM
  - 高端设备：Android 13.0，8GB+ RAM

- **基准版本**：
  - 优化前版本：v3.2.0
  - 优化后版本：v3.3.0

- **测试工具**：
  - Android Profiler
  - Battery Historian
  - Firebase Performance Monitoring
  - LeakCanary

## 1. 内存占用比较

### 测试用例 1.1：空闲状态内存占用
1. **测试方法**：
   - 分别安装优化前后版本
   - 启动应用，等待30秒
   - 使用Android Profiler记录基础内存占用
   - 重复5次取平均值
2. **评估指标**：
   - 基础内存占用（MB）
   - 内存占用波动范围

### 测试用例 1.2：设备搜索内存占用
1. **测试方法**：
   - 启动设备搜索并持续1分钟
   - 记录整个过程的内存曲线
   - 特别关注峰值内存和搜索结束后的内存恢复情况
2. **评估指标**：
   - 峰值内存占用（MB）
   - 搜索结束后30秒的内存占用（MB）
   - 临时内存分配频率

### 测试用例 1.3：媒体播放过程内存占用
1. **测试方法**：
   - 连接设备并播放高清视频5分钟
   - 期间操作：暂停、继续、调整音量、跳转进度
   - 记录整个过程的内存曲线
2. **评估指标**：
   - 平均内存占用（MB）
   - 操作期间内存波动范围
   - 播放结束后的内存恢复情况

## 2. 响应时间比较

### 测试用例 2.1：设备搜索响应时间
1. **测试方法**：
   - 测量从调用startDiscovery()到发现第一个设备的时间
   - 测量从调用startDiscovery()到设备列表稳定的时间
   - 在相同网络环境下重复10次
2. **评估指标**：
   - 首个设备发现时间（ms）
   - 设备列表稳定时间（ms）
   - 每秒发现设备数量

### 测试用例 2.2：操作响应时间
1. **测试方法**：
   - 测量以下操作的响应时间：
     - 连接设备
     - 播放开始
     - 暂停响应
     - 音量调整
     - 进度跳转
   - 每个操作重复10次取平均值
2. **评估指标**：
   - 各操作的平均响应时间（ms）
   - 操作超时率（%）

### 测试用例 2.3：API调用链分析
1. **测试方法**：
   - 使用Firebase Performance Monitoring记录关键API调用链耗时
   - 关注网络请求时间和本地处理时间
2. **评估指标**：
   - 各API调用的耗时分布
   - 瓶颈点分析

## 3. 电池消耗比较

### 测试用例 3.1：静态电池消耗
1. **测试方法**：
   - 应用启动但不执行操作，保持在后台1小时
   - 使用Battery Historian记录电池消耗
2. **评估指标**：
   - 每小时电池消耗百分比
   - 唤醒锁使用情况

### 测试用例 3.2：活跃使用电池消耗
1. **测试方法**：
   - 模拟用户场景：搜索设备-连接-播放-控制-断开，循环30分钟
   - 记录电池消耗及CPU/网络活动
2. **评估指标**：
   - 每小时电池消耗百分比
   - CPU使用时间
   - 网络请求数量和数据量

### 测试用例 3.3：后台播放电池消耗
1. **测试方法**：
   - 连接设备并开始播放
   - 将应用切换到后台1小时
   - 记录电池消耗
2. **评估指标**：
   - 每小时电池消耗百分比
   - 后台网络活动频率

## 4. 启动性能比较

### 测试用例 4.1：冷启动时间
1. **测试方法**：
   - 完全关闭应用并清除后台
   - 测量从点击图标到应用可交互的时间
   - 使用Android Vitals记录
   - 重复10次取平均值
2. **评估指标**：
   - 冷启动时间（ms）
   - 启动过程中的内存分配

### 测试用例 4.2：热启动时间
1. **测试方法**：
   - 将应用切换到后台
   - 等待1分钟
   - 测量重新切回应用的时间
   - 重复10次取平均值
2. **评估指标**：
   - 热启动时间（ms）
   - 状态恢复完整性

## 5. 资源利用率比较

### 测试用例 5.1：CPU使用率
1. **测试方法**：
   - 在设备搜索、媒体播放等关键场景记录CPU使用率
   - 对比优化前后的CPU峰值和平均使用率
2. **评估指标**：
   - 平均CPU使用率（%）
   - CPU峰值持续时间

### 测试用例 5.2：网络资源使用
1. **测试方法**：
   - 记录设备搜索、媒体播放等场景的网络请求数量和数据量
   - 使用Charles Proxy或类似工具记录网络活动
2. **评估指标**：
   - 总网络请求数量
   - 数据传输量（KB）
   - 重复请求比率

### 测试用例 5.3：缓存命中率
1. **测试方法**：
   - 利用新增的缓存统计功能记录各类缓存的命中率
   - 对比优化前的估算命中率
2. **评估指标**：
   - 设备缓存命中率（%）
   - HTTP响应缓存命中率（%）
   - 服务描述缓存命中率（%）

## 测试结果分析方法

为全面评估性能改进效果，我们将采用以下分析方法：

1. **百分比改进**：计算每项指标的改进百分比
2. **统计显著性**：使用T检验确认改进是否有统计显著性
3. **用户体验影响**：分析性能变化对用户体验的实际影响
4. **设备差异**：分析不同性能设备上的改进幅度差异

## 测试结果记录模板

| 指标 | 优化前 | 优化后 | 改进 | 改进率 | 统计显著性 |
|-----|-------|-------|-----|-------|---------|
| 空闲内存占用 (MB) | | | | | |
| 设备搜索峰值内存 (MB) | | | | | |
| 首个设备发现时间 (ms) | | | | | |
| 冷启动时间 (ms) | | | | | |
| 电池消耗 (%/h) | | | | | |
| 设备缓存命中率 (%) | | | | | |

## 结论和建议

测试完成后，根据各项指标的综合表现，得出以下结论：

1. 最显著的改进领域
2. 仍需进一步优化的方面
3. 在不同设备上的性能差异
4. 对后续版本的优化建议 